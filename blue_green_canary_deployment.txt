Advantages of Deployment---
One deployment already there in production. After few days client assks for new deployment . THis requirmement developers will write code based on new requirement. 
WIthout downtime we have to move these new requirement, into production.
We can use differeent deployment strategies--
1. Rolling update and rollback
2. Blue and Green Deployment
3. Canary Deployment

Link: https://thenewstack.io/deployment-strategies/
--------------------------------------
Rolling Update and Rollback-
==================
[root@ip-172-31-44-228 ~]# yum install docker -y && service docker start

[root@ip-172-31-44-228 ~]# git clone https://github.com/Naresh240/RollingUpdate-Rollback-NodejsApp.git

[root@ip-172-31-44-228 RollingUpdate-Rollback-NodejsApp]# docker login

[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# docker build -t maheshroshini08/nodejs-helloworld:v1 .

[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# docker images
REPOSITORY                          TAG       IMAGE ID       CREATED         SIZE
maheshroshini08/nodejs-helloworld   v1        6a03c9216591   8 seconds ago   914MB

[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# docker push maheshroshini08/nodejs-helloworld:v1

Now i can see the image in docker hub. 

Currently in the server.js file we have the display as "hello world" for v1, client as asked for a change and we have updated as "hello world...." for v2 ***

  res.send('Hello World');   to   res.send('Hello World......!');  in 
[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# cat server.js

Now we are build it again with v2****


[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# docker build -t maheshroshini08/nodejs-helloworld:v2 .

Again we will push the same to docker hub------

[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# docker push maheshroshini08/nodejs-helloworld:v2

[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# cat deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodejs-deployment
  labels:
    app: nodejs
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0%
      maxSurge: 1
  selector:
    matchLabels:
      app: nodejs
  template:
    metadata:
      labels:
        app: nodejs
    spec:
      containers:
      - name: nodejs-deployment
        image: maheshroshini08/nodejs-helloworld:v1
        imagePullPolicy: Always
        ports:
        - containerPort: 8080


[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# kubectl apply -f deployment.yml --record

[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# kubectl rollout status deployment nodejs-deployment
Waiting for deployment "nodejs-deployment" rollout to finish: 0 of 2 updated replicas are available...
Waiting for deployment "nodejs-deployment" rollout to finish: 1 of 2 updated replicas are available...

[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# kubectl rollout history deployment nodejs-deployment
deployment.apps/nodejs-deployment
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=deployment.yml --record=true

[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# cat service.yml
---
apiVersion: v1
kind: Service
metadata:
  name: nodejs-service
spec:
  type: LoadBalancer
  selector:
    app: nodejs
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# kubectl apply -f service.yml
nodejs-service   LoadBalancer   10.100.184.137   a796fd6713dbc46bda55da75cf6a1c16-2109254087.us-east-2.elb.amazonaws.com   8080:32087/TCP   3s


CHeck in Loadbalanacer in EC2 and wait till both the nodes are in service***

http://a796fd6713dbc46bda55da75cf6a1c16-2109254087.us-east-2.elb.amazonaws.com:8080/  -- now i can see

Hello World


Now let's upgrade the image to v2***


[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# kubectl set image deployment nodejs-deployment nodejs-deployment=maheshroshini08/nodejs-helloworld:v2
deployment.apps/nodejs-deployment image updated
[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# kubectl rollout history deployment nodejs-deployment
deployment.apps/nodejs-deployment
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=deployment.yml --record=true
2         kubectl apply --filename=deployment.yml --record=true

NOw we if chck with same LB, we will see the new output ---- Hello Word ...!

[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# kubectl rollout history deployment nodejs-deployment --revision=2
deployment.apps/nodejs-deployment with revision #2
Pod Template:
  Labels:       app=nodejs
        pod-template-hash=c474c7b78
  Annotations:  kubernetes.io/change-cause: kubectl apply --filename=deployment.yml --record=true
  Containers:
   nodejs-deployment:
    Image:      maheshroshini08/nodejs-helloworld:v2
    Port:       8080/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>


[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# kubectl rollout undo deployment nodejs-deployment --to-revision=1
deployment.apps/nodejs-deployment rolled back
[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# kubectl rollout status deployment nodejs-deployment
deployment "nodejs-deployment" successfully rolled out
[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# kubectl rollout history deployment nodejs-deployment
deployment.apps/nodejs-deployment
REVISION  CHANGE-CAUSE
2         kubectl apply --filename=deployment.yml --record=true
3         kubectl apply --filename=deployment.yml --record=true

[root@ip-172-31-41-248 RollingUpdate-Rollback-NodejsApp]# kubectl rollout history deployment nodejs-deployment --revision=3
deployment.apps/nodejs-deployment with revision #3
Pod Template:
  Labels:       app=nodejs
        pod-template-hash=6f479cd5f
  Annotations:  kubernetes.io/change-cause: kubectl apply --filename=deployment.yml --record=true
  Containers:
   nodejs-deployment:
    Image:      maheshroshini08/nodejs-helloworld:v1

===============================================================================================================

Blue-Green Deployment::
-------------------------------

[root@ip-172-31-41-248 ~]# git clone https://github.com/Naresh240/Blue-Green-Deployment-NodejsApp.git


Here we use 2 deployemnt and 2 service files***


[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# cat deployment-blue.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodejs-deployment-blue
  labels:
    app: nodejs
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nodejs
      version: v1
  template:
    metadata:
      labels:
        app: nodejs
        version: v1
    spec:
      containers:
      - name: nodejs-blue
        image: maheshroshini08/nodejs-helloworld:v1
        ports:
        - containerPort: 8080
[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# cat service-blue.yml
---
apiVersion: v1
kind: Service
metadata:
  name: nodejs-service-blue
spec:
  type: LoadBalancer
  selector:
    app: nodejs
    version: v1
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080

Similary change image name with "v2" in deployment-green.yml file**** 


[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# cat deployment-green.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodejs-deployment-green
  labels:
    app: nodejs
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nodejs
      version: v2
  template:
    metadata:
      labels:
        app: nodejs
        version: v2
    spec:
      containers:
      - name: nodejs-green
        image: maheshroshini08/nodejs-helloworld:v2
        ports:
        - containerPort: 8080

[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl apply -f deployment-blue.yml
deployment.apps/nodejs-deployment-blue created
[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl apply -f service-blue.yml
service/nodejs-service-blue created


http://a7ca951a45bf34acf911fd2bdd67421f-79412995.us-east-2.elb.amazonaws.com:8080/  -->output : hello World

Let's deploy the same with ingress...

[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/rbac-role.yaml
clusterrole.rbac.authorization.k8s.io/alb-ingress-controller created
clusterrolebinding.rbac.authorization.k8s.io/alb-ingress-controller created
serviceaccount/alb-ingress-controller created

NOw need to create service account --- copy IAM policy arn which we created  to the below content

alb-ingress-controller-policy - arn:aws:iam::305286667209:policy/alb-ingress-controller-policy



[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# eksctl create iamserviceaccount \
>     --region us-east-2 \
>     --name alb-ingress-controller \
>     --namespace kube-system \
>     --cluster eksdemo1 \
>     --attach-policy-arn arn:aws:iam::305286667209:policy/alb-ingress-controller-policy \
> --override-existing-serviceaccounts \
>     --approve


[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# eksctl  get iamserviceaccount --cluster eksdemo1 --region us-east-2
2021-06-28 17:36:51 [ℹ]  eksctl version 0.54.0
2021-06-28 17:36:51 [ℹ]  using region us-east-2
NAMESPACE       NAME                    ROLE ARN
kube-system     alb-ingress-controller  arn:aws:iam::305286667209:role/eksctl-eksdemo1-addon-iamserviceaccount-kube-Role1-TJBZ1DN3O8C3

------------------------------------
Deploy ALB Ingress Controller

[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/alb-ingress-controller.yaml

-------------------------------
Verify Deployment

[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl get deploy -n kube-system
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
alb-ingress-controller   0/1     1            0           43s

---------------------------
Edit ALB Ingress Controller Manifest:

[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl edit deploy alb-ingress-controller -n kube-system

    spec:
      containers:
      - args:
        - --ingress-class=alb
        - --cluster-name=eksdemo1

[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl get deploy -n kube-system
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
alb-ingress-controller   1/1     1            1           2m28s


[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# cat ingress.yml
---
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: nodejs-ingress
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          serviceName: nodejs-service-blue  **** this is the important part, to change traffice we make it "green" *****
          servicePort:  8080

1st we deploy with """blue traffic""", 


[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl apply -f ingress.yml
ingress.networking.k8s.io/nodejs-ingress created
[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl get ingress
NAME             CLASS    HOSTS   ADDRESS   PORTS   AGE
nodejs-ingress   <none>   *                 80      5s

[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl get pods -n kube-system
NAME                                      READY   STATUS    RESTARTS   AGE
alb-ingress-controller-7f699ff874-dhrl2   1/1     Running   0          6m7s

[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl logs alb-ingress-controller-7f699ff874-dhrl2 -n kube-system

E0628 17:46:33.400632       1 controller.go:217] kubebuilder/controller "msg"="Reconciler error" "error"="failed to build LoadBalancer configuration due to failed to resolve 2 qualified subnet with at least 8 free IP Addresses for ALB. Subnets must contains these tags: 'kubernetes.io/cluster/eksdemo1': ['shared' or 'owned'] and 'kubernetes.io/role/elb': ['' or '1']


Now go to subnets of this cluster, add those values to  vpc and public subnets and add kubernetes.io/cluster/eksdemo1 owned  and kubernetes.io/role/elb 1

[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl delete -f ingress.yml
[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl apply -f ingress.yml

[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl get ingress
NAME             CLASS    HOSTS   ADDRESS                                                                  PORTS   AGE
nodejs-ingress   <none>   *       6ad0ef5b-default-nodejsing-3ba2-1161380364.us-east-2.elb.amazonaws.com   80      49s


Now if we check in LB, we can see this ALB,
http://6ad0ef5b-default-nodejsing-3ba2-1161380364.us-east-2.elb.amazonaws.com/  we can see the URL*** Hello World


If we check the LB: http://a7ca951a45bf34acf911fd2bdd67421f-79412995.us-east-2.elb.amazonaws.com:8080/ we can see Hello World**


WE ARE GIVING 100% TRAFFIC TO BLUE DEPLOYMENT *****************************


[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl apply -f deployment-green.yml
[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl apply -f service-green.yml


[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl get deploy
NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
nodejs-deployment-blue    2/2     2            2           44m
nodejs-deployment-green   2/2     2            2           68s
[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl get svc
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)          AGE
kubernetes             ClusterIP      10.100.0.1       <none>                                                                   443/TCP          113m
nodejs-service-blue    LoadBalancer   10.100.205.245   a7ca951a45bf34acf911fd2bdd67421f-79412995.us-east-2.elb.amazonaws.com    8080:30504/TCP   42m
nodejs-service-green   LoadBalancer   10.100.185.175   a94f40682077841f39d7773a91904483-502944106.us-east-2.elb.amazonaws.com   8080:31076/TCP   18s

Now if we check with 2nd Classic Loadbalancer http://a94f40682077841f39d7773a91904483-502944106.us-east-2.elb.amazonaws.com:8080/  it is up and showing output Hello World......!

Let's switch the traffic to new LB..... by changing the content in the ""ingress.yml""" file***


Before:
        backend:
          serviceName: nodejs-service-blue
After:
        backend:
          serviceName: nodejs-service-green

[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# vi ingress.yml
[root@ip-172-31-41-248 Blue-Green-Deployment-NodejsApp]# kubectl apply -f ingress.yml

Now if i check the ALB, we can see the output- http://6ad0ef5b-default-nodejsing-3ba2-1161380364.us-east-2.elb.amazonaws.com/ 

Hello World......!
======================================================================================================================================================

Canary Deployment::
---------------------------------

Entire Naresh's kubernetes docs are available here:: https://github.com/Naresh240/kubernetes.git

In this check for "deployment-strategies", we can get canary deployment

[root@ip-172-31-45-186 ~]# git clone https://github.com/Naresh240/kubernetes.git

[root@ip-172-31-45-186 deployment-strategies]# pwd
/root/kubernetes/deployment-strategies
[root@ip-172-31-45-186 deployment-strategies]# ls
Blue-Green-Deployment  Canary-Deployment  RollingUpdate-Rollback

[root@ip-172-31-45-186 Canary-Deployment]# pwd
/root/kubernetes/deployment-strategies/Canary-Deployment
[root@ip-172-31-45-186 Canary-Deployment]# ls
deployment-v1.yml  Dockerfile        ingress.yml      package.json  server.js
deployment-v2.yml  external-dns.yml  istio-rules.yml  README.md     service.yml


We have 2 deployment files, but we have only one "service file"" ... 

Here we are going to manage the traffic by using ""istio-rules"""". In this we use "weight" to move the traffic from one deployment to another deployment***

Pre-requisties:
===================
- Install GIT
- EKS Cluster
- Install ALB-Ingress-Controller
- Install Istio
- Request a Cerficate using Certificate Manager
- Create Hosted Zone with our Domain Name
- External DNS Setup

Istio is designed for extensibility and can handle a diverse range of deployment needs. Istio's control plane runs on Kubernetes, and you can add applications deployed in that cluster to your mesh, extend the mesh to other clusters, or even connect VMs or other endpoints running outside of Kubernetes.



Intsalling Istio:   https://github.com/Naresh240/Istio-Installation
================================
1.
Before we can get started configuring Istio we’ll need to first install the command line tools that you will interact with.

echo 'export ISTIO_VERSION="1.5.2"' >> ${HOME}/.bash_profile
source ${HOME}/.bash_profile
mkdir environment
cd ~/environment
curl -L https://istio.io/downloadIstio | sh -

[root@ip-172-31-45-186 ~]# echo 'export ISTIO_VERSION="1.5.2"' >> ${HOME}/.bash_profile
[root@ip-172-31-45-186 ~]# source ${HOME}/.bash_profile
[root@ip-172-31-45-186 ~]# mkdir environment
[root@ip-172-31-45-186 ~]# cd ~/environment
[root@ip-172-31-45-186 environment]# curl -L https://istio.io/downloadIstio | sh -

Istio has been successfully downloaded into the istio-1.5.2 folder on your system.
[root@ip-172-31-45-186 environment]# ls
istio-1.5.2


2. The installation directory contains: *Installation YAML files for Kubernetes in install/kubernetes *Sample applications in samples/ *The istioctl client binary in the bin/ directory (istioctl is used when manually injecting Envoy as a sidecar proxy).

[root@ip-172-31-45-186 environment]# cd ${HOME}/environment/istio-${ISTIO_VERSION}
[root@ip-172-31-45-186 istio-1.5.2]# sudo cp -v bin/istioctl /usr/bin/
‘bin/istioctl’ -> ‘/usr/bin/istioctl’
[root@ip-172-31-45-186 istio-1.5.2]# istioctl version --remote=false
1.5.2


3. Istio will be installed in the istio-system namespace. ***

[root@ip-172-31-45-186 istio-1.5.2]# istioctl manifest apply --set profile=demo

[root@ip-172-31-45-186 istio-1.5.2]# kubectl -n istio-system get svc
NAME                        TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)                                                                                                                                      AGE
grafana                     ClusterIP      10.100.166.231   <none>                                                                   3000/TCP                                                                                                                                     9s
istio-egressgateway         ClusterIP      10.100.10.111    <none>                                                                   80/TCP,443/TCP,15443/TCP                                                                                                                     11s
istio-ingressgateway        LoadBalancer   10.100.182.194   abff420aa1a35413ab8124c78732a166-608716309.us-east-2.elb.amazonaws.com   15020:30874/TCP,80:30322/TCP,443:32172/TCP,15029:30829/TCP,15030:31062/TCP,15031:32538/TCP,15032:30110/TCP,31400:31552/TCP,15443:32301/TCP   11s
istio-pilot                 ClusterIP      10.100.246.111   <none>                                                                   15010/TCP,15011/TCP,15012/TCP,8080/TCP,15014/TCP,443/TCP                                                                                     28s
istiod                      ClusterIP      10.100.114.201   <none>                                                                   15012/TCP,443/TCP                                                                                                                            28s
jaeger-agent                ClusterIP      None             <none>                                                                   5775/UDP,6831/UDP,6832/UDP                                                                                                                   9s
jaeger-collector            ClusterIP      10.100.243.227   <none>                                                                   14267/TCP,14268/TCP,14250/TCP                                                                                                                9s
jaeger-collector-headless   ClusterIP      None             <none>                                                                   14250/TCP                                                                                                                                    9s
jaeger-query                ClusterIP      10.100.154.59    <none>                                                                   16686/TCP                                                                                                                                    9s
kiali                       ClusterIP      10.100.66.152    <none>                                                                   20001/TCP                                                                                                                                    9s
prometheus                  ClusterIP      10.100.77.150    <none>                                                                   9090/TCP                                                                                                                                     9s
tracing                     ClusterIP      10.100.252.154   <none>                                                                   80/TCP                                                                                                                                       9s
zipkin                      ClusterIP      10.100.199.196   <none>                                                                   9411/TCP                                                                                                                                     9s


[root@ip-172-31-45-186 istio-1.5.2]# kubectl -n istio-system get pods
NAME                                    READY   STATUS    RESTARTS   AGE
grafana-57cb8b8d44-zfl84                1/1     Running   0          92s
istio-egressgateway-c5bc4f66-47dv6      1/1     Running   0          93s
istio-ingressgateway-7c4d7d88d4-czmbj   1/1     Running   0          93s  ** in our case we need this*******
istio-tracing-7fcc6f5848-bm7bh          1/1     Running   0          91s
istiod-64878565f8-kwdmd                 1/1     Running   0          110s
kiali-6875bdf78-cck6q                   1/1     Running   0          91s
prometheus-d8d9467dd-gbpbg              2/2     Running   0          91s

By default it will create so many services and pods. ***


We will use our above cloned repo::
[root@ip-172-31-45-186 ~]# git clone https://github.com/Naresh240/kubernetes.git

[root@ip-172-31-45-186 Canary-Deployment]# pwd
/root/kubernetes/deployment-strategies/Canary-Deployment

We will use our 2 images with different versions** v1 and v2--- in our 2 deployment files****

maheshroshini08/nodejs-helloworld:v1 and maheshroshini08/nodejs-helloworld:v2

[root@ip-172-31-45-186 Canary-Deployment]# pwd
/root/kubernetes/deployment-strategies/Canary-Deployment

[root@ip-172-31-45-186 Canary-Deployment]# cat service.yml
---
apiVersion: v1
kind: Service
metadata:
  name: nodejs
  labels:
    app: nodejs
spec:
  type: LoadBalancer
  selector:
    app: nodejs
  ports:
  - name: http
    port: 8080

Note:: HERE WE ARE HAVING ONLY ONE SERVICE YML FILE AND USING SAME "SELECTOR LABEL" IN BOTH THE DEPLOYMENTS******

Here as we have only one service.yml file, service file will get confuse which deployment to connect as they have same selector***

So to resolve this we use "istio-rules****************** "

[root@ip-172-31-45-186 Canary-Deployment]# kubectl apply -f deployment-v1.yml
deployment.apps/nodejs-v1 created
[root@ip-172-31-45-186 Canary-Deployment]# kubectl apply -f deployment-v2.yml
deployment.apps/nodejs-v2 created
[root@ip-172-31-45-186 Canary-Deployment]# kubectl apply -f ingress.yml
ingress.networking.k8s.io/nodejs-ingress configured
[root@ip-172-31-45-186 Canary-Deployment]# kubectl apply -f service.yml
service/nodejs created

[root@ip-172-31-45-186 Canary-Deployment]# kubectl apply -f istio-rules.yml
gateway.networking.istio.io/nodejs-gateway created
virtualservice.networking.istio.io/nodejs created
destinationrule.networking.istio.io/nodejs created


[root@ip-172-31-45-186 Canary-Deployment]# kubectl get deploy
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
nodejs-v1   1/1     1            1           41s
nodejs-v2   1/1     1            1           35s

[root@ip-172-31-45-186 Canary-Deployment]# kubectl get ingress
NAME             CLASS    HOSTS   ADDRESS                                                                  PORTS   AGE
nodejs-ingress   <none>   *       6ad0ef5b-default-nodejsing-3ba2-1244134030.us-east-2.elb.amazonaws.com   80      121m

[root@ip-172-31-45-186 Canary-Deployment]# kubectl get svc
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP                                                              PORT(S)          AGE
kubernetes   ClusterIP      10.100.0.1      <none>                                                                   443/TCP          158m
nodejs       LoadBalancer   10.100.191.99   ad2f39578b7874d229abea838e5b58c2-114466821.us-east-2.elb.amazonaws.com   8080:30664/TCP   46s

NOw we can see the output of the v1: --- 

http://ad2f39578b7874d229abea838e5b58c2-114466821.us-east-2.elb.amazonaws.com:8080/  Hello WOrld


[root@ip-172-31-45-186 Canary-Deployment]# cat istio-rules.yml
  gateways:
  - nodejs-gateway
  http:
  - route:
    - destination:
        host: nodejs   ---> this is the service file name we are reffering
        subset: v1
      weight: 80
    - destination:
        host: nodejs  --> this is the service file name we are referering
        subset: v2
      weight: 20

##################################
kind: Service
metadata:
  name: nodejs
  labels:
    app: nodejs


As the istio-rules in 80 and 20 we are not able to see the differenece, so we will make it as 50:50


[root@ip-172-31-45-186 Canary-Deployment]# kubectl apply -f istio-rules.yml
gateway.networking.istio.io/nodejs-gateway unchanged
virtualservice.networking.istio.io/nodejs configured   *************AFTER THE CHANGES AND RE-APPLY WE GOT THIS ONLY CHANGED***
destinationrule.networking.istio.io/nodejs unchanged


Now, lets see the traffic in URL, we can see both the versions every now and then whenever we refresh the page***

After 100% traffic movement happen to v2 in istio-rules.yml file, we will delete the deployment v1 ************

i.e.,

   - destination:
        host: nodejs
        subset: v1
      weight: 00
    - destination:
        host: nodejs
        subset: v2
      weight: 100


[root@ip-172-31-45-186 Canary-Deployment]# kubectl delete -f deployment-v1.yml
deployment.apps "nodejs-v1" deleted

Now if i check the URL is pointing completely to v2 *****
